{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create default pipeline\n",
    "Pipeline is initilized with BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998706579208374},\n",
       " {'label': 'POSITIVE', 'score': 0.9998013377189636},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997760653495789},\n",
       " {'label': 'NEGATIVE', 'score': 0.999782145023346},\n",
       " {'label': 'POSITIVE', 'score': 0.9509405493736267}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "reviews = [\n",
    "    \"The movie was great!\",\n",
    "    \"The movie was okay.\",\n",
    "    \"The movie was terrible...\",\n",
    "    \"The movie was so bad that I had to leave the theater. I will never watch it again.\",\n",
    "    \"Netflix's The Witcher is a dark fantasy epic that will make you want to toss a coin of your own to the show's creators.\",\n",
    "]\n",
    "\n",
    "classifier(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the model\n",
    "Tokenizer must be set to the same one that was used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.6459116339683533},\n",
       " {'label': '3 stars', 'score': 0.7961868047714233},\n",
       " {'label': '1 star', 'score': 0.7816266417503357},\n",
       " {'label': '1 star', 'score': 0.7096598744392395},\n",
       " {'label': '4 stars', 'score': 0.44803386926651}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this course, we will teach you how to use a variety of programming languages to create and manipulate\n",
      "In this course, we will teach you how to write functional HTML5 code using JavaScript with HTML5\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "generated_text = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=20,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "\n",
    "for text in generated_text:\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998706579208374},\n",
       " {'label': 'POSITIVE', 'score': 0.9998013377189636},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997760653495789},\n",
       " {'label': 'NEGATIVE', 'score': 0.999782145023346},\n",
       " {'label': 'POSITIVE', 'score': 0.9509405493736267}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer) # if tokenizer is not specified, it will use the default model tokenizer\n",
    "\n",
    "classifier(reviews) # the results are the same as before because \"distilbert-base-uncased-finetuned-sst-2-english\" is a default model for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722]\n",
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a transformer network is simple\"\n",
    "\n",
    "res = tokenizer(sequence)\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "encoded_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_tokens)\n",
    "\n",
    "orignal_text = tokenizer.decode(ids)\n",
    "\n",
    "print(res) # dictionary with input_ids (with SOS and EOS tokens), token_type_ids, and attention_mask\n",
    "print(tokens)\n",
    "print(ids) # token ids without SOS and EOS tokens\n",
    "print(decoded_tokens)\n",
    "print(orignal_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face with TensorFLow\n",
    "`TFAutoModel` is a generic class that can be used to load any pre-trained model from the transformers library, while `TFAutoModelForSequenceClassification` is a specific class designed for sequence classification tasks that includes a classification head on top of the base model. You can import the same pre-trained model using both classes, but the resulting models will differ in their architecture and functionality. `TFAutoModel` gives you the base model without any additional task-specific heads, while `TFAutoModelForSequenceClassification` gives you the base model with an additional classification head for sequence classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both methods load the same pre-trained model, `TFAutoModelForSequenceClassification` provides a higher level of abstraction and can be used to load any pre-trained sequence classification model from the transformers library. `TFDistilBertForSequenceClassification` is a lower-level class that is specific to the DistilBERT architecture. This means that you can only use this class to load pre-trained DistilBERT models for sequence classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_419']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "X_train = [\n",
    "    \"I've been waiting for this LEGO Witcher series for a long time! I'm so excited!\",\n",
    "    \"The Witch series on Netflix is so inacurate. I can't believe they made it.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 24), dtype=int32, numpy=\n",
       "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  2023, 23853,\n",
       "         6965,  2121,  2186,  2005,  1037,  2146,  2051,   999,  1045,\n",
       "         1005,  1049,  2061,  7568,   999,   102],\n",
       "       [  101,  1996,  6965,  2186,  2006, 20907,  2003,  2061, 27118,\n",
       "        10841, 11657,  1012,  1045,  2064,  1005,  1056,  2903,  2027,\n",
       "         2081,  2009,  1012,   102,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 24), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"tf\") # If max_length is not specified, it will use the longest sequence in the X_train\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using the model\n",
    "with tf.GradientTape() as tape:\n",
    "    logits = model(batch)[0]\n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    preds = tf.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7832916  4.0378428]\n",
      " [ 3.5807006 -2.9227448]]\n",
      "[[4.0100541e-04 9.9959904e-01]\n",
      " [9.9850404e-01 1.4960265e-03]]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(logits.numpy())\n",
    "print(probs.numpy())\n",
    "print(preds.numpy()) # 0 is negative and 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9995989203453064},\n",
       " {'label': 'NEGATIVE', 'score': 0.9985040426254272}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "classifier(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved the same results using `pipeline` and `TFAutoModelForSequenceClassification`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at models were not used when initializing TFDistilBertForSequenceClassification: ['dropout_419']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at models and are newly initialized: ['dropout_439']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'models'\n",
    "\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.save_pretrained(save_dir)\n",
    "\n",
    "our_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "our_model = TFAutoModelForSequenceClassification.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9995989203453064},\n",
       " {'label': 'NEGATIVE', 'score': 0.9985040426254272}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_classifier = pipeline('sentiment-analysis', model=our_model, tokenizer=our_tokenizer)\n",
    "\n",
    "our_classifier(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usuall workflow:**\n",
    "1. Prepare our dataset\n",
    "2. Load pre-trained Tokenizer, call it with our dataset to get the encodings\n",
    "3. Build TensorFlow dataset object with encodings\n",
    "4. Load pre-trained model\n",
    "5. Load Trainer and train int\n",
    "6. Use native TensorFlow training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments('test-trainer')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
